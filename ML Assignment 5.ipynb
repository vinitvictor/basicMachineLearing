{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "from sklearn import model_selection\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "colName=['sepal_length','sepal_width','petal_length','petal_width','class']\n",
    "irisDataset=pd.read_csv(\"irisD.data\",names=colName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width           class\n",
       "0             5.1          3.5           1.4          0.2     Iris-setosa\n",
       "1             4.9          3.0           1.4          0.2     Iris-setosa\n",
       "2             4.7          3.2           1.3          0.2     Iris-setosa\n",
       "3             4.6          3.1           1.5          0.2     Iris-setosa\n",
       "4             5.0          3.6           1.4          0.2     Iris-setosa\n",
       "..            ...          ...           ...          ...             ...\n",
       "145           6.7          3.0           5.2          2.3  Iris-virginica\n",
       "146           6.3          2.5           5.0          1.9  Iris-virginica\n",
       "147           6.5          3.0           5.2          2.0  Iris-virginica\n",
       "148           6.2          3.4           5.4          2.3  Iris-virginica\n",
       "149           5.9          3.0           5.1          1.8  Iris-virginica\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irisDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>5.843333</td>\n",
       "      <td>3.057333</td>\n",
       "      <td>3.758000</td>\n",
       "      <td>1.199333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>0.828066</td>\n",
       "      <td>0.435866</td>\n",
       "      <td>1.765298</td>\n",
       "      <td>0.762238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>4.300000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>1.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>6.400000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>7.900000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sepal_length  sepal_width  petal_length  petal_width\n",
       "count    150.000000   150.000000    150.000000   150.000000\n",
       "mean       5.843333     3.057333      3.758000     1.199333\n",
       "std        0.828066     0.435866      1.765298     0.762238\n",
       "min        4.300000     2.000000      1.000000     0.100000\n",
       "25%        5.100000     2.800000      1.600000     0.300000\n",
       "50%        5.800000     3.000000      4.350000     1.300000\n",
       "75%        6.400000     3.300000      5.100000     1.800000\n",
       "max        7.900000     4.400000      6.900000     2.500000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irisDataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=irisDataset.iloc[:,:-1].values \n",
    "Y=irisDataset.iloc[:,4].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelEncoder=LabelEncoder()\n",
    "Y=labelEncoder.fit_transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 2, 1, 1, 2, 0, 2, 0, 2, 1, 2, 0, 0, 2, 0, 1, 2, 1, 1, 2,\n",
       "       2, 0, 1, 1, 1, 0, 2, 2, 1, 1, 0, 0, 0, 2, 1, 0, 1, 2, 1, 2, 0, 1,\n",
       "       1, 0, 0, 0, 2, 0, 2, 2, 0, 2, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 2, 0, 0, 1, 2, 2, 2, 2, 2, 2, 1, 2,\n",
       "       2, 2, 0, 2, 0, 0, 0, 2, 1, 1, 2, 2, 1, 0, 2, 0, 1, 2, 0, 1, 1, 2,\n",
       "       0, 0, 2, 2, 1, 1, 1, 2, 2, 0, 2, 0, 1, 0, 0, 2, 2, 2, 0, 0, 2, 1,\n",
       "       1, 2, 0, 2, 0, 0, 2, 2, 0, 0, 0, 2, 1, 0, 1, 2, 0, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shuffling the dataset\n",
    "X,Y=shuffle(X,Y,random_state=20)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "foldModel=model_selection.KFold(n_splits=5,random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Train Set--------------------\n",
      "[[5.7 3.  4.2 1.2]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.1 3.5 1.4 0.2]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.  2.  3.5 1. ]\n",
      " [6.  3.  4.8 1.8]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [6.7 3.  5.  1.7]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [5.  3.  1.6 0.2]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.7 2.8 4.1 1.3]]\n",
      "--------------------Test Set--------------------\n",
      "[[4.6 3.2 1.4 0.2]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.  2.2 5.  1.5]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [5.9 3.  5.1 1.8]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [6.3 2.8 5.1 1.5]]\n",
      "--------------------Train Set--------------------\n",
      "[[4.6 3.2 1.4 0.2]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.  2.2 5.  1.5]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [5.9 3.  5.1 1.8]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.  2.  3.5 1. ]\n",
      " [6.  3.  4.8 1.8]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [6.7 3.  5.  1.7]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [5.  3.  1.6 0.2]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.7 2.8 4.1 1.3]]\n",
      "--------------------Test Set--------------------\n",
      "[[5.7 3.  4.2 1.2]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.1 3.5 1.4 0.2]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.5 2.4 3.8 1.1]]\n",
      "--------------------Train Set--------------------\n",
      "[[4.6 3.2 1.4 0.2]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.  2.2 5.  1.5]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [5.9 3.  5.1 1.8]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.1 3.5 1.4 0.2]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [6.7 3.  5.  1.7]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [5.  3.  1.6 0.2]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.7 2.8 4.1 1.3]]\n",
      "--------------------Test Set--------------------\n",
      "[[6.2 2.9 4.3 1.3]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.  2.  3.5 1. ]\n",
      " [6.  3.  4.8 1.8]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [7.2 3.6 6.1 2.5]]\n",
      "--------------------Train Set--------------------\n",
      "[[4.6 3.2 1.4 0.2]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.  2.2 5.  1.5]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [5.9 3.  5.1 1.8]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.1 3.5 1.4 0.2]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.  2.  3.5 1. ]\n",
      " [6.  3.  4.8 1.8]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [6.7 3.  5.  1.7]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [5.  3.  1.6 0.2]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.7 2.8 4.1 1.3]]\n",
      "--------------------Test Set--------------------\n",
      "[[4.4 2.9 1.4 0.2]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [5.  3.5 1.6 0.6]]\n",
      "--------------------Train Set--------------------\n",
      "[[4.6 3.2 1.4 0.2]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.  2.2 5.  1.5]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [5.9 3.  5.1 1.8]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.1 3.5 1.4 0.2]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.  2.  3.5 1. ]\n",
      " [6.  3.  4.8 1.8]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [5.  3.5 1.6 0.6]]\n",
      "--------------------Test Set--------------------\n",
      "[[6.9 3.2 5.7 2.3]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [6.7 3.  5.  1.7]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [5.  3.  1.6 0.2]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.7 2.8 4.1 1.3]]\n"
     ]
    }
   ],
   "source": [
    "# trainset and testset\n",
    "for i,j in foldModel.split(X,Y):\n",
    "    train_x,test_x=X[i],X[j]\n",
    "    train_y,test_y=Y[i],Y[j]\n",
    "    print(\"-\"*20+\"Train Set\"+\"-\"*20)\n",
    "    print(train_x)\n",
    "    print(\"-\"*20+\"Test Set\"+\"-\"*20)\n",
    "    print(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9333333333333333, 0.9666666666666667, 0.9666666666666667, 0.9666666666666667, 0.9333333333333333]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['DecisionTreeClassifierModel.joblib']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decisionTreeAccuracy=[]\n",
    "#Decision tree classifier\n",
    "for i,j in foldModel.split(X,Y):\n",
    "    trainingX,testingX=X[i],X[j]\n",
    "    trainingY,testingY=Y[i],Y[j]\n",
    "    \n",
    "    clfDTree=DecisionTreeClassifier()\n",
    "    clfDTree.fit(trainingX,trainingY)\n",
    "    predY=clfDTree.predict(testingX)\n",
    "    decisionTreeAccuracy.append(accuracy_score(predY,testingY))\n",
    "print(decisionTreeAccuracy)\n",
    "# saving model to run later\n",
    "joblib.dump(clfDTree,\"DecisionTreeClassifierModel.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate how K impacts the overall accuracy of kNN on the dataset. Use histogram plots to visualize the results and identify the best K."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy for each value of K\n",
      "[0.96, 0.9400000000000001, 0.9666666666666666, 0.9666666666666666, 0.9733333333333333, 0.9800000000000001, 0.9800000000000001, 0.9666666666666666, 0.9666666666666668, 0.9800000000000001, 0.9733333333333334, 0.9800000000000001, 0.9800000000000001, 0.9800000000000001, 0.9733333333333334, 0.9733333333333334, 0.9800000000000001, 0.9733333333333334, 0.9733333333333334, 0.9800000000000001, 0.9733333333333334, 0.96, 0.96, 0.9466666666666667, 0.96]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['KNNClassifierModel.joblib']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Knn Classifier\n",
    "maxKvalue=25\n",
    "KNNAccuracy=[[] for i in range(maxKvalue)]\n",
    "meankNNaccuracy = []\n",
    "\n",
    "for i,j in foldModel.split(X,Y):\n",
    "    trainingKX,testingKX=X[i],X[j]\n",
    "    trainingKY,testingKY=Y[i],Y[j]\n",
    "\n",
    "    for k in range(0,maxKvalue):\n",
    "        clfknn=KNeighborsClassifier(n_neighbors=k+1)\n",
    "        clfknn.fit(trainingKX,trainingKY)\n",
    "        predKY=clfknn.predict(testingKX)\n",
    "        KNNAccuracy[k].append(accuracy_score(predKY,testingKY))\n",
    "# calculating mean accuarcy for each k value\n",
    "for i in range(0,maxKvalue):\n",
    "    meankNNaccuracy.append(np.mean(KNNAccuracy[i]))\n",
    "\n",
    "print(\"Mean accuracy for each value of K\")\n",
    "print(meankNNaccuracy)\n",
    "# saving model to run later\n",
    "joblib.dump(clfknn,\"KNNClassifierModel.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best value of K for better accuracy= 6\n"
     ]
    }
   ],
   "source": [
    "# best value of K\n",
    "bestKvalue=meankNNaccuracy.index(max(meankNNaccuracy))+1\n",
    "print(\"Best value of K for better accuracy=\",bestKvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ans : - Best value for k is 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ans: - KNN Accuracy Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'KNN Accuracy')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "knnAccuracy = {\n",
    "    'KNN K Value': [y+1 for y in range(maxKvalue)], \n",
    "    'Accuracy': meankNNaccuracy\n",
    "}\n",
    "df=pd.DataFrame(knnAccuracy)\n",
    "graph = sns.barplot(x=\"KNN K Value\",y=\"Accuracy\",data=df,color=\"red\")\n",
    "graph.set_ylim([.9,1])\n",
    "graph.set_title(\"KNN Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the overall accuraciesof kNN with the best K and decision trees using histogram plots. Which classifier is better and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1, 'KNN Classifier Accuracy Vs Decision Tree Classifier Accuracy')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# comparing the average accuracy of KNN and Decision Tree\n",
    "graphModel = {\n",
    "    'Classifier': ('KNN Classifier', 'Decision Tree Classifier'), \n",
    "    'Accuracy': [meankNNaccuracy[bestKvalue], np.mean(decisionTreeAccuracy)]\n",
    "}\n",
    "df = pd.DataFrame(graphModel)\n",
    "x = sns.barplot(x=\"Classifier\",y=\"Accuracy\",data=df)\n",
    "x.set_ylim([.9,1])\n",
    "graph.set_title(\"KNN Classifier Accuracy Vs Decision Tree Classifier Accuracy\" ,loc=\"center\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy= 98.00000000000001\n",
      "Decision Tree Accuracy= 95.33333333333334\n"
     ]
    }
   ],
   "source": [
    "print(\"KNN Accuracy=\",meankNNaccuracy[bestKvalue]*100)\n",
    "print(\"Decision Tree Accuracy=\",np.mean(decisionTreeAccuracy)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: - \n",
    "From above observation, we can see KNN performs better than decision tree. \n",
    "## KNN Accuracy= 98.00000000000001\n",
    "## Decision Tree Accuracy= 95.33333333333334\n",
    "KNN is a lazy learning classifier. Lazy learning, which means that there is no explicit training phase before classification. Instead, any attempts to generalize or abstract the data is made upon classification. KNN classifications can be computationally expensive as the algorithm parse through all data points for each classification.  For this reason, KNN tends to work best on smaller datasets that do not have many features. \n",
    "\n",
    "On the other hand, Decision tree is an Eager learning classifier. Decision tree needs a bigger dataset to predict values with better accuracy. With a bigger dataset, we need to take care of overfitting because it affects the efficiency of the Decision tree. \n",
    "\n",
    "Thats why KNN performs better than Decision Tree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading model from system and running 50 random samples from iris dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 0, 0, 1, 2, 0, 0, 2, 0, 2, 1, 2, 2, 2, 2, 0, 0, 0,\n",
       "       0, 2, 1, 0, 2, 2, 2, 2, 2, 2, 1, 0, 2, 0, 1, 2, 1, 1, 1, 2, 0, 1,\n",
       "       2, 1, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting random 50 samples from dataset to check saved model\n",
    "df=pd.DataFrame(irisDataset).sample(n=50,random_state=999)\n",
    "sampleX=df.iloc[:,:-1].values\n",
    "groundTruth=labelEncoder.fit_transform(df.iloc[:,4].values)\n",
    "groundTruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=None, splitter='best')\n"
     ]
    }
   ],
   "source": [
    "dtree=joblib.load(\"DecisionTreeClassifierModel.joblib\")\n",
    "print(dtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 0, 0, 1, 2, 0, 0, 2, 0, 2, 1, 2, 2, 2, 2, 0, 0, 0,\n",
       "       0, 2, 2, 0, 2, 2, 2, 2, 2, 2, 1, 0, 2, 0, 1, 2, 1, 1, 1, 2, 0, 1,\n",
       "       2, 1, 0, 2, 0, 0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtree.predict(sampleX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=25, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn=joblib.load(\"KNNClassifierModel.joblib\")\n",
    "knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 2, 0, 2, 1, 2, 2, 2, 2, 0, 0, 0,\n",
       "       0, 2, 2, 0, 2, 2, 2, 2, 2, 2, 1, 0, 2, 0, 1, 2, 1, 1, 1, 2, 0, 1,\n",
       "       2, 1, 0, 2, 0, 0])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.predict(sampleX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "link to code--------- https://colab.research.google.com/drive/1h9DrLL3ut8UgVoU4tUWqP3De3EysZb7g"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
